{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "714451ae",
   "metadata": {},
   "source": [
    "# Quantization Aware Policy Optimization\n",
    "\n",
    "### Random Quantization (Unbiased but impractical)\n",
    "\n",
    "Quantized parameters $\\phi \\sim \\Phi_\\theta$ are sampled from a distribution parameterized by the true parameters $\\theta$. We want to maximize the following objective:\n",
    "\n",
    "$$J(\\theta) = \\mathbb E_{\\tau \\sim \\pi_\\phi, \\phi \\sim \\Phi_\\theta}[r(\\tau)] \n",
    "= \\int \\sum_\\phi P(\\phi|\\theta)\\pi_\\phi(\\tau)r(\\tau)d\\tau.$$\n",
    "\n",
    "$\\tau$ is a trajectory (LLM output), $\\pi_\\phi$ is the quantized policy (quantized LLM), and $r$ is the reward function (grade of LLM output).\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\int \\sum_\\phi P(\\phi|\\theta)\\pi_\\phi(\\tau)r(\\tau)d\\tau \n",
    "= \\mathbb E_{\\tau \\sim \\pi_\\phi, \\phi \\sim \\Phi_\\theta}\\left[\\frac{\\nabla_\\theta P(\\phi|\\theta)}{P(\\phi|\\theta)}r(\\tau)\\right].$$\n",
    "\n",
    "This approach is unrealistic and impractical for the following reasons:\n",
    "- In practice, quantized parameters are not random.\n",
    "- Estimating $\\nabla_\\theta J(\\theta)$ is challenging because it requires sampling both $\\phi$ and $\\tau$.\n",
    "\n",
    "### Deterministic Quantization with Surrogate Gradients (More practical but biased)\n",
    "\n",
    "If quantization is deterministic, $\\nabla_\\theta P(\\phi|\\theta) = 0$ or is undefined, so we cannot optimize using $\\nabla_\\theta J(\\theta)$ described previously. \n",
    "\n",
    "Now, \n",
    "$$\\phi = \\varphi(\\theta) = \\frac{\\operatorname{round}(s(\\theta - z))}{s} + z,$$\n",
    "and \n",
    "$$J(\\theta) = \\mathbb E_{\\tau \\sim \\pi_{\\varphi(\\theta)}}[r(\\tau)] \n",
    "= \\int \\pi_{\\varphi(\\theta)}(\\tau)r(\\tau)d\\tau.$$\n",
    "\n",
    "Since $\\operatorname{round}$ is not differentiable, we will use a surrogate, straight-through estimate of its gradient, \n",
    "$$\\tilde\\nabla_x\\operatorname{round}(x) = 1.$$\n",
    "\n",
    "Then,\n",
    "$$\\tilde\\nabla_\\theta\\varphi(\\theta) = 1,$$\n",
    "so\n",
    "$$\\tilde\\nabla_\\theta f(\\varphi(\\theta)) = \\nabla_{\\varphi(\\theta)} f(\\varphi(\\theta))\\tilde\\nabla_\\theta\\varphi(\\theta) = \\nabla_{\\varphi(\\theta)} f(\\varphi(\\theta)).$$\n",
    "\n",
    "Finally,\n",
    "$$\\tilde\\nabla_\\theta J(\\theta) = \\tilde\\nabla_\\theta \\int \\pi_{\\varphi(\\theta)}(\\tau)r(\\tau)d\\tau\n",
    "= \\mathbb E_{\\tau \\sim \\pi_{\\varphi(\\theta)}}\\left[\n",
    "    \\frac{\\tilde\\nabla_\\theta\\pi_{\\varphi(\\theta)}(\\tau)}{\\pi_{\\varphi(\\theta)}(\\tau)}r(\\tau)\n",
    "\\right]\n",
    "= \\mathbb E_{\\tau \\sim \\pi_{\\varphi(\\theta)}}\\left[\\frac{\\nabla_{\\varphi(\\theta)}\\pi_{\\varphi(\\theta)}(\\tau)}{\\pi_{\\varphi(\\theta)}(\\tau)}r(\\tau)\\right].$$\n",
    "\n",
    "\n",
    "### Augmented GRPO Objective\n",
    "\n",
    "To reduce variance and add regularization, we can optimize a GRPO style objective:\n",
    "\n",
    "$$J(\\theta) = \\mathbb E_{\\tau \\sim \\pi_{\\varphi(\\theta)}}\\left[\n",
    "    \\min\\left(\n",
    "        \\frac{\\pi_{\\varphi(\\theta)}(\\tau)}{\\pi_{\\phi}(\\tau)}\\hat A(\\tau),\n",
    "        \\operatorname{clip}\\left(\n",
    "            \\frac{\\pi_{\\varphi(\\theta)}(\\tau)}{\\pi_{\\phi}(\\tau)}, \n",
    "            1 - \\epsilon, \n",
    "            1 + \\epsilon\n",
    "        \\right)\\hat A(\\tau)\n",
    "    \\right) - \\beta \\mathbb D_{\\text{KL}}[\\pi_{\\varphi(\\theta)}\\|\\pi_\\text{ref}]\n",
    "\\right].$$\n",
    "\n",
    "$\\phi = \\varphi(\\theta)$, but it is treated as a constant for gradient computation.\n",
    "\n",
    "The normalized advantage $\\hat A(\\tau)$ is given by\n",
    "$$\\hat A(\\tau) = \\frac{\n",
    "    r(\\tau) - \\mathbb E_{\\tau \\sim \\pi_{\\varphi(\\theta)}}[r(\\tau)]\n",
    "}{\n",
    "    \\sqrt{\\operatorname{Var}_{\\tau \\sim \\pi_{\\varphi(\\theta)}}[r(\\tau)]}\n",
    "}.$$\n",
    "\n",
    "Note, if $\\pi_\\text{ref}$ is not quantized, $\\mathbb D_{\\text{KL}}[\\pi_{\\varphi(\\theta)}\\|\\pi_\\text{ref}]$ is a standard QAT training objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a03ed1",
   "metadata": {},
   "source": [
    "### Compare newest commit to last working commit\n",
    "https://github.com/ethanewer/qapo/compare/2d7a2a51c480f0bf4628ab10e3720bbcc38a1c79...0960e31f688be5bc07ab364cc646501a3b3ccc4c\n",
    "\n",
    "### QAT Mini-batch Forward Times (Qwen2.5-0.5B)\n",
    "- Computing QAT params without optimization: 49s\n",
    "- Computing QAT params with optimization: \n",
    "\n",
    "### Good notebook image\n",
    "\n",
    "- 171503962539.dkr.ecr.ap-northeast-2.amazonaws.com/dlt-kubeflow-codeserver:torch2.3.0-cuda12.4.0-py310-ubuntu2204"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11168c2c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
